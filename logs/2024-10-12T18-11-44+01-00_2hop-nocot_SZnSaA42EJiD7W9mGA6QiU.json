{
  "version": 2,
  "status": "error",
  "eval": {
    "run_id": "BBnHyBoxmMieCTbmSQcUaq",
    "created": "2024-10-12T18:11:44+01:00",
    "task": "2hop_nocot",
    "task_id": "SZnSaA42EJiD7W9mGA6QiU",
    "task_version": 0,
    "task_attribs": {},
    "task_args": {},
    "dataset": {
      "samples": 10,
      "shuffled": true
    },
    "model": "together/meta-llama/Llama-3-8b-chat-hf",
    "model_base_url": "https://api.together.xyz/v1",
    "model_args": {},
    "config": {
      "max_samples": 25,
      "max_tasks": 25,
      "max_subprocesses": 25
    },
    "revision": {
      "type": "git",
      "origin": "ghp:tomekkorbak/latent_reasoning.git",
      "commit": "47bf777"
    },
    "packages": {
      "inspect_ai": "0.3.32"
    }
  },
  "plan": {
    "name": "plan",
    "steps": [
      {
        "solver": "system_message",
        "params": {
          "template": "Answer the following question directly and concisely, without any reasoning. There is always an answer. If the answer is ambiguous, use your best guess."
        }
      },
      {
        "solver": "generate",
        "params": {
          "cache": "CachePolicy"
        }
      }
    ],
    "config": {
      "temperature": 0.0
    }
  },
  "stats": {
    "started_at": "2024-10-12T18:11:44+01:00",
    "completed_at": "2024-10-12T18:11:44+01:00",
    "model_usage": {}
  },
  "error": {
    "message": "NameError(\"name 'graded_sample' is not defined\")",
    "traceback": "Traceback (most recent call last):\n\n  File \"/Users/mikita/miniconda3/envs/latent/lib/python3.10/site-packages/inspect_ai/_eval/task/run.py\", line 260, in task_run\n    sample_results = await asyncio.gather(*sample_coroutines)\n\n  File \"/Users/mikita/miniconda3/envs/latent/lib/python3.10/site-packages/inspect_ai/_eval/task/run.py\", line 435, in task_run_sample\n    await scorer(state, Target(sample.target)) if scorer else None\n\n  File \"/Users/mikita/projects/latent_reasoning/latent_reasoning/evaluate_llama_incontext.py\", line 49, in custom_grader\n    return Score(value=CORRECT if graded_sample.correct else INCORRECT)\n\nNameError: name 'graded_sample' is not defined. Did you mean: 'create_sample'?\n",
    "traceback_ansi": "\u001b[31m┌─\u001b[0m\u001b[31m────────────────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────────────────────────\u001b[0m\u001b[31m─┐\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/Users/mikita/miniconda3/envs/latent/lib/python3.10/site-packages/inspect_ai/_eval/task/\u001b[0m\u001b[1;33mrun.py\u001b[0m:\u001b[94m260\u001b[0m in \u001b[92mtask_run\u001b[0m       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/Users/mikita/miniconda3/envs/latent/lib/python3.10/site-packages/inspect_ai/_eval/task/\u001b[0m\u001b[1;33mrun.py\u001b[0m:\u001b[94m435\u001b[0m in                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92mtask_run_sample\u001b[0m                                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/Users/mikita/projects/latent_reasoning/latent_reasoning/\u001b[0m\u001b[1;33mevaluate_llama_incontext.py\u001b[0m:\u001b[94m49\u001b[0m in \u001b[92mcustom_grader\u001b[0m             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 46 \u001b[0m\u001b[2m│   │   \u001b[0mfirst_hop_reference_answer=state.metadata.get(\u001b[33m\"\u001b[0m\u001b[33manswer_intermediate\u001b[0m\u001b[33m\"\u001b[0m),                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 47 \u001b[0m\u001b[2m│   │   \u001b[0mprompt=state.input_text,                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 48 \u001b[0m\u001b[2m│   \u001b[0m)                                                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m> \u001b[0m 49 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m Score(value=CORRECT \u001b[94mif\u001b[0m graded_sample.correct \u001b[94melse\u001b[0m INCORRECT)                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 50 \u001b[0m                                                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 51 \u001b[0m                                                                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 52 \u001b[0m\u001b[1;95m@scorer\u001b[0m(metrics=[accuracy(), stderr()])                                                                        \u001b[31m│\u001b[0m\n\u001b[31m└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\u001b[0m\n\u001b[1;91mNameError: \u001b[0mname \u001b[32m'graded_sample'\u001b[0m is not defined\n"
  }
}